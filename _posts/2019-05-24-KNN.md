---
title: "A Brief Introduction to k-Nearest Neighbors"
date: 2019-05-24
tags: [concepts, machine learning, classification]
mathjax: true
classes: wide
---

## The Types of Problems that $$k$$-Nearest Neighbors Can Solve

k-Nearest Neighbors (referred to as KNN) is a method used for **classification** problems. KNN is a **supervised** learning algorithm because it predicts based on a-priori known input and output pairs. KNN is appropriate to situations where we have $$n$$ numerical variables and 1 or more categorical variables. Here are some examples of situations which are appropriate for KNN:

1. A bank has collected credit scores and monthly salaries of its customers. They also know whether these customers have taken out a loan with the bank or not. They wish to predict whether new customers, given their credit score and monthly salary are going to take out a loan or not.

2. A High School has collected data on their past students. These include: GPA (grade point average), number of absences per year, standardized testing scores, number of years at the school and the categorical data is whether upon graduation, they went onto university, a technical degree, work or unemployment. They wish to predict what path students close to graduating will follow.

3. The NHS (National Health Service in the UK) has historical data for children such as: birth weight, days early or late according to expected birth date, mother and fathers age at birth, mothers' weight, fathers' weight and the categorical data of whether the child was overweight or not at the age of 2. They wish to provide support to the parents of children who are predicted to be overweight later in life.

4. Reddit has collected data on posts which are age of the posters account and percentage of the post that is made up of punctuation. They have categorised a few posts as spam or not spam and they want to flag future posts based on these variables.

The last example is the one I will use in my explanation of the method. It is chosen because there are only 2 variables which may be plotted in the Cartesian plane and we can colour the points depending on whether they are spam or not.

## Visualising the Method

Consider the situation below. Reddit has categorised 10 posts as spam or not spam and have given the age of the account posting (in years) and the percentage of the post that is punctuation.

<img src="{{ site.url }}{{ site.baseurl }}/images/knn/spam1.png" alt="10 Reddit Posts Categorised as Spam or Not Spam">

In this dataset we can see that posts from newer members with lots of punctuation are most likely spam. There is also a grey, new post that is yet to be categorised. I am sure you agree that given this plot, it fits the pattern of a spam post. $$k$$-Nearest neighbors uses your intuition in a formal way. Here is what it does:

1. Find the $$k$$ nearest points to the new datapoint based on a distance metric (discussed later).
2. Assign to the new datapoint the category that is most common amongst the $$k$$ nearest points.

Therefore, in our example if $$k = 4$$, we look at the points in pink below and as 3 are spam, to 1 not spam, we assign the category of spam to the new point.

<img src="{{ site.url }}{{ site.baseurl }}/images/knn/spam2.png" alt="10 Reddit Posts Categorised as Spam or Not Spam">

I think this should come across as intuitive to most people and is applicable in any number of dimensions, so long as the variables are numerical and hence, a distance can be computed from them to the new point.

You may be wondering how to choose the value of $$k$$. Usually, the best idea is to try different values of $$k$$, working out validation accuracy for each one and then select the best performing $$k$$ (Link to train/validate/test to be added here).

## Remarks

It is very important that before KNN is performed that the variables are **normalized**. This means that they are centered around a mean of 0 and are dilated to have a standard deviation of 1. This is important because if your variables have vastly differing orders of magnitude (see example number 1), the variable with greater magnitude is essentially going to make the classification decision alone. This is often called scaling and most libraries will do it for you, but It is always worth checking. The transformation that will do this is:

$$Z = \frac{X - \mu}{\sigma}$$

where $$\mu$$ is the mean and $$\sigma$$ is the standard deviation of the variable. This should be done to every variable separately for your data.

KNN can take a **very long time to run**. This is because in the standard implementation, for every new data point you want to classify you must calculate its distance from every data point in your dataset. However, there are ways to speed this up such as caching distances or not necessarily calculating every single distance by perhaps just sampling the full set.

It is possible to perform KNN on non-numerical data, but you will need to define a way of measuring distance between the points that makes sense in the context. This could be assigning a numerical value from 1 to 5 for categories or something more complex.

## The Distance Metric

KNN can be run using any way of measuring distance in space. The most widely known way to measure distance in 2-dimensional space is the 2-norm, or Euclidean distance. For points $$a = (x_1, y_1)$$ and $$b = (y_1, y_2)$$ the distance between them is:

$$d(a, b) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$

which is a result derived from Pythagoras' theorem. This idea can be generalised to define the $p$-norm in $$n$$-dimensional space for $$a = (a_1, a_2, ... , a_n)$$ and $$b = (b_1, b_2, ... , b_n)$$

$$d_p(a, b) = \left((b_1 - a_1)^2 + (b_2 - a_2)^2 + ... + (b_n - a_n)^2\right)^{\frac{1}{p}}$$

In the case of $$p = 2$$ this is just Euclidean distance, but we get different notions of distance for other $$p$$-values. When $$p = 1$$ we get what is referred to as Manhattan distance (or taxicab length for my days in university). In this case, the distance between two points is the sum of the absolute differences of their coordinates or for $$a = (a_1, a_2, ... , a_n)$$ and $$b = (b_1, b_2, ... , b_n)$$:

$$d_p(a, b) = |b_1 - a_1| + |b_2 - a_2| + ... + |b_n - a_n|$$

It is referred to as the Manhattan distance as to travel between points we must move parallel to an axis. Similar to the block system in New York City.

You may be wondering why you would ever use this notion of distance for KNN. If you use Manhattan distance in your predictor then it will be **less** sensitive to errors as the value of the exponent $$p = 1$$. If you know that your dataset has quite a few outliers, but you want the predictor to fit to them, try using the Manhattan distance. In general, the higher the $$p$$ value in your distance norm, the more sensitive to outliers your model will be.

## Weights

In the standard implementation of KNN described above, once we have found the $k$-nearest neighbors to a new point they are all treated equally when assigning the category to the new point. However, does this really make sense? What if one of them is far away? Shouldn't it contribute less to the classification? This can be done by weighting each points contribution to the classification. A common approach is to scale each contribution by $$\frac{1}{d^2}$$ where $$d$$ is the distance from the new point. This will mean points very far away will contribute little to the classification.