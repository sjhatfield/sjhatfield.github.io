---
title: "Predicting Football Games That Are Worth Watching"
date: 2019-05-21
tags: [machine learning, analytics, classification]
mathjax: true
classes: wide
---

## The Problem

I am a passionate football (soccer[^1]) player and a semi-passionate football viewer. Whilst I do enjoy watching football games with friends, many times that I watch a football game at home to relax I get bored or distracted. Whilst I love playing football, I only find some games enjoyable to watch. Usually, I try to hedge my bets with football viewing and watch the two most highly placed teams in the Premier League (EPL) table on the date of the fixture. However, I have lost count of the number of times I have stayed awake past my bedtime to watch two highly regarded teams play a boring 0-0 draw. So, the ideal outcome for this project is:

**Given the list of fixtures taking place next weekend and the current form of all the teams in the league, predict which will games will be exciting to watch**

Games will be classified as exciting if:

1. There are 4 goals scored in the game.
2. There are 3 goals scored in the game *and* each team had at least 6 shots on target *and* there were goals in both halves.
3. There are 2 goals scored in the game *and*
    * the home and away team had 8 or more shots on target each *or*
    * the home and away team got at least 4 yellow cards each.

These criteria are based on my personal preference. In my opinion, games with goals are exciting. Games with shots on target are also exciting because it means that goal scoring chances are being created and at least one team is attacking a lot. I have required both teams to have many shots on target as this means that it is not one-sided. If there are lots of yellow cards in a game it means that the game is fiesty and may be a rivalry which are often exciting games.

### The Ideal Outcome

The ideal output of this project will be a page on this blog which tells the user without having them enter any data what upcoming game is the most likely to be worth watching. If my this model can predict with over 50% accuracy an exciting game to watch then I will consider it a success. I feel at the moment that I predict the exciting games to watch about 20% of the time. Why so low a prediction you may be thinking. Well we are effectively trying to predict the outcome of a sporting event which has a lot of random chance involved.

### Formulation as a Machine Learning Problem

Let's now formulate this problem as a machine learning (ML) problem. This problem can be viewed either as a classification or regression problem. I am going to begin by treating it as a classification problem. This will prove not to be too successful and I think I will treat it as a regression problem also in a later post. The ML problem at hand is: given past EPL game data which have been classified as worth watching or not (a binary classifier) and a game set to take place in the near future with the teams current performance in the league, predict the classification (worth watching or not) of the game. I am going to first use k-Nearest Neighbors to classify upcoming games.

## Data Collection and Analysis

First, we must collect data, clean it and perform some basic analysis. This [website](http://www.football-data.co.uk/englandm.php) provides csv files of each EPL season going back all the way to 1993-94. **I have used the seasons from 2000-01 until 2017-18 inclusive.** The full jupyter notebook with all the data loading, cleaning, analysis and wrangling can be [viewed here](https://github.com/sjhatfield/worthwatching/blob/master/EPL_Wrangling_Analysis.ipynb) on my GitHub account. I will just present the findings on this page.

The dataset contains statistics for over 6000 EPL games. The statistics provided aside from teams, date, referee and so on, are related to goals, corners, fouls, cards and then betting odds. Some interesting facts found from the dataset are:

* The top scoring teams in the EPL are in order: Man Utd, Arsenal, Chelsea, Liverpool and Man City. The only semi-surprise here is Man City as they have only performed well in the EPL relatively recently.
* The lowest scoring teams are: Huddersfield, Bradford, Derby, Cardiff and Sheffield Utd. Derby stick out here as they have taken part in more seasons than the others.
* The teams that have been relagated and promoted the most in order are: West Brom, Birmingham, Hull, Norwich, Burnley, Middlesbrough and Sunderland.
* There is only one team that score less on average at home than away: Coventry. This is a surprising statistic although they have had issues with their home stadium.
* The highest scoring win away from home was by Tottenham against Hull in the 2016-17 season.

We can see the advantage of playing at home on goals scored in the histogram below:
<img src="{{ site.url }}{{ site.baseurl }}/images/EPL1/home_away.png" alt="Histogram showing goals scored home and away">

We can also see an increase in the average number of goals being scored per game over the years. There is variance over the years but an r-value of $0.509$ indicates that there is a positive trend of medium strength.

<img src="{{ site.url }}{{ site.baseurl }}/images/EPL1/goals.png" alt="Scatter plot of average number of goals scored over each season">

### Preparing the Data for the Model

The criteria for determining whether a game was worth watching was written as a function and then applied to each row in the dataframe to create a new column, *'watch'*. If this column has 1 as an entry the game is worth watching, otherwise it has a 0.

```python
def worth_watching(row):
    four_cond = (row['FTHG'] + row['FTAG'] >= 4)

    three_goals = (row['FTHG'] + row['FTAG'] == 3)
    each_half = (0 < row['HTHG'] + row['HTAG'] <= 2)
    three_shots = (row['HST'] >= 6 and row['AST'] >= 6)
    three_cond = three_goals and each_half and three_shots

    two_goals = (row['FTHG'] + row['FTAG'] == 2)
    two_shots = (row['HST'] >= 8 and row['AST'] >= 8)
    yellows = (row['HY'] >= 4 and row['AY'] >= 4)
    two_cond = two_shots and (two_shots or yellows)

    return four_cond or three_cond or two_cond

df['watch'] = df.apply(lambda r: 1 if worth_watching(r) else 0, axis=1)
```

Here are the definitions of the columns in the dataframe:

* FTHG/FTAG - full-time home/away goals
* HTHG/HTAG - half-time home/away goals
* HST/AST - home/away shots on target
* HY/AY - home/away yellow cards

It was found that 33.8% of games fulfill one of these criteria. I was somewhat surprised by that because I felt the criteria were fairly strong and would be rare. Evidently, the way I have been selecting games up to now is pretty subpar!

Next, is to decide what statistics will be fed into the model to determine whether a future game will be worth watching or not. Of course, these must be avaiable before the game takes place. I thought about trying to use the betting statistics which are given as home win odds, away win odds and draw odds. But these will give more of an impression of whether a game has gone according to plan or is an upset. Whilst this does impact a games excitement, these concepts have not been implemented in my criteria.

I decided that to predict a game, I would use various statistics of each team, over the games that have already taken place in that season. The averages I will compute are:

* HGS/AGS - average home/away goals scored per game.
* HYC/AYV - average home/away yellow cards per game.
* HRC/ARC - average home/away red cards per game.
* HWW/AWW - home/away proportion of games worth watching so far.

For example, if West Brom are playing Man Utd in the **5th** game of the season, I will predict the outcome using the averages above of their first **4** games.

*This does mean that my model will not be able to predict the first game of the season for each team and that it should get more accurate as the season progresses.*

Whilst the database had the statistics to compute these averages it was quite the challenge to actually find them. The code can be viewed in the Jupyter Notebook [here in code cell number 31](https://github.com/sjhatfield/worthwatching/blob/master/EPL_Wrangling_Analysis.ipynb). It was very satisfying when it finally worked!

Now that these were found it was simply a case of drop NA values from the beginning of the season and save the data to a csv file so I can reopen it in the model notebook.

### $k$-Nearest Neighbors

I am going to make a seperate post briefly explaining k-Nearest Neighbors. The link will appear here once I have written it!

## Implementing the Model

If you would like to go straight to the code please follow [this link](https://github.com/sjhatfield/worthwatching/blob/master/EPL_Wrangling_Analysis.ipynb) to the notebook.

I decided to implement the k-nearest neighbors model myself just using Pandas and NumPy as a fun challenge. The first step for the model to execute is to normalize all the columns of the dataframe. This means that one column may not have a big impact on the classification due to it's magnitude being vastly different. This doesn't apply so much to my data but imagine one column being salary and one being number of years employed at a company. The salary column would be many orders of magnitude larger than the years and would stop the years having much of an impact on the output.

Normalizing uses built-in functions of Pandas and concept of standardizing a variable which I have taught many times to my High School mathematics students.

```python
def normalize(df):
    return (df - df.mean()) / df.std()
```

The next step was to implement a function to compute the Euclidean distance from a new point to all the points in the dataframe. This turns out to be the bottleneck on speed when I try to fit the model on my dataset, as this needs to be done for each new point to be classified. I added some assertions to help with debugging later.

```python
def find_distances(df, newpoint):
    assert type(newpoint) == pd.Series, 'The new point must be a pd.Series object'
    assert newpoint.shape[0] == (df.shape[1] - 1), f'Your point is the wrong shape'
    distances = []
    for i, row in df.drop('watch', axis=1).iterrows():
        distances.append(np.linalg.norm(row - newpoint))
    return pd.Series(distances)
```

Next is a function that given the original data, the distances found above and a value for $k$, will return the $k$ rows with the smallest distances. These are the $k$ closest points to the new one.

```python
def nearest(df, distances, k):
    assert type(k) == int, 'k must be an integer'
    assert 1 <= k <= df.shape[0], 'k must be a positive integer'
    assert type(distances) == pd.Series, 'Distances must be a pd.Series object'
    copy = df.copy()
    copy['distances'] = distances
    return copy.sort_values(by='distances',axis=0)[:k].drop('distances', axis=1)
```

The function below takes the $k$-rowed output of the one above and gives a classification of the new point based on the majority ruling. If the majority of the closest rows are worth watching then the new point is worth watching. In the case of a draw I decided to err on the side of caution and classify the game as not worth watching.

```python
def worth_watching(nearest):
    average = nearest['watch'].sum() / nearest['watch'].count()
    return int(average > 0.5)
```

Finally, we put all the pieces together and predict the classification of a new point given the original data and a value of k.

```python
def predict(dataframe, newpoint, k):
    df = normalize(dataframe)
    # We must apply the same normalizing function to the new point
    normed_point = (newpoint - dataframe.mean()) / dataframe.std()
    # Drop the column it has picked up from the normalization
    normed_point.drop('watch', inplace=True)
    dists = find_distances(df, normed_point)
    near = nearest(df, dists, k)
    return worth_watching(near)
```

I did some quick testing in the notebook and it seemed to be working!

## Choosing The $k$ Value

There is not really a training phase for $k$-nearest neighbors as there are no parameters to be fitted or regression coefficients to be found. We need to find out the optimal value of k for prediction. I am going to use $10$-fold cross validation for this. But first, I must split the data into the validation set and testing set. Ordinarily, we would use a split like $70%/30%$ or $80%/20%$, but due to how slowly my implementation runs I will be using $10%$ for validation and $5%$ for testing. Later we will use the full dataset.

At some point I will write a post about training, validating and testing and the link will appear here.

Once we have the validation set, we split it into $10$ subsets. Then, taking each of the $10$ parts in turn, we predict the $10%$ of the validation set using the remaining 90%. This is done 10 times keeping a running total of the number of correct predicitons. Then we have an accuracy for the current value of $k$. We repeat this for the values of $k$ we want to assess and the one with the highest accuracy is selected.

The final step, is the predict the testing data using all the validation data and the chosen value of $k$, to get a final accuracy. This last step ensures that any bias in the validation data which the $k$-value may have fitted has been removed from our final accuracy measure.

Now I will present the process described above in Python code. Firstly, splitting the data into validation and testing.

```python
random.seed(2718) # Euler
validation_frac = 0.1 # Train on over 600 games
validation_frac = 0.05 # Test on over 300 games
indices_validate = random.sample([i for i in range(len(data))], 
                              math.floor(validation_frac * len(data)))
indices_test = random.sample([i for i in range(len(data)) if i not in indices_validate], 
                             math.floor(testing_frac * len(data)))
data_validate = data.iloc[indices_validate].reset_index()
data_test = data.iloc[indices_test].reset_index()
```

Now the validation stage:

```python
accuracy = []
for k in range(1, 20):
    start = time.time()
    correct = 0
    for i in range(10):
        validate_indices_low = math.floor(i / 10 * len(data_train))
        validate_indices_high = math.floor((i + 1) / 10 * len(data_train))
        data_validate = data_train.iloc[range(validate_indices_low, 
                                              validate_indices_high)]
        data_fit = data_train.iloc[[i for i in range(len(data_train)) 
                                    if i not in range(validate_indices_low, 
                                                      validate_indices_high)]]
        watch_pred = pd.DataFrame(data_validate['watch'])
        watch_pred['pred'] = data_validate.apply(lambda row: predict(data_fit, row.drop('watch'), k), axis=1)
        correct += len(watch_pred[watch_pred['watch'] == watch_pred['pred']])
        print(f'{i} fold done for k={k}', end='. ')
    accuracy.append(correct / len(data_train))
    end = time.time()
    print(f'\nThe validation accuracy for k={k} is {round(accuracy[k - 1] * 100, 3)}% and took {end - start} seconds to run')
```

The above loop over $19$ $k$-values, took around an hour to run. It found the optimum $k$-value to be $10$ with a validation accuracy of $67.8\%$.

Finally, we can assess the classifier performance on the testing data by running:

```python
watch_pred = pd.DataFrame(data_test['watch'])
watch_pred['pred'] = data_test.apply(lambda row: predict(data_train, row.drop('watch'), final_k), axis=1)
correct = len(watch_pred[watch_pred['watch'] == watch_pred['pred']])
accuracy = correct / len(data_test)
print(f'The accuracy on the testing data for k={final_k} is {round(accuracy * 100, 3)}')
```

where final_k is the $k$-value found from the validation process.

**This gave a final accuracy of $65.4\%$**. This may sound low but we are in a sense, predicting the outcome of a sporting event which has a huge element of chance involved in the outcome. If predicting sporting events was easy then a lot of people would make money gambling on them!

As can be seen in the code snippet above I also found values from the confusion matrix. If I truly value my time, I should be concerned with false positives. In this situation the model will tell me to watch a game but it will actually be dull.

Obviously, taking an hour to find the optimal value of $k$ is not ideal and we should try to speed this process up. Using [Scikit-Learn](https://scikit-learn.org/) (a machine learning library) in Python will do just that.

## Choosing The $k$-value Using Scikit-Learn



[^1]: Despite being married to a US citizen I am not get Americanized enough to refer to football as soccer. I am sure it will happen though.